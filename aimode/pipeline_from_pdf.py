# -*- coding: utf-8 -*-
"""
ä» PDF ä¸€æ¡é¾™è·‘å®Œé€‰è¯æµç¨‹ï¼Œå¹¶æŠŠæ‰€æœ‰ç»“æœæ”¾è¿›ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼š
  reading_{æ–‡ç« å}

æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
  1) --mode ai
       ç”¨ AI è‡ªåŠ¨é€‰è¯ï¼ˆä¸éœ€è¦æ‰‹å·¥ select è¯è¡¨ï¼‰
  2) --mode list
       ç”¨ä½ è‡ªå·±å‡†å¤‡å¥½çš„ select è¯è¡¨ï¼ˆå’Œä¹‹å‰ä¸€æ ·ï¼‰
"""

import argparse
import json
import math
from pathlib import Path
from collections import Counter

from extract_pdf_text import extract_text_from_pdf
from build_vocab_combined import build_vocab
from csv_to_json import csv_to_json
from ai_select_wordlist import ai_select_words_for_article
from config import DEFAULT_TOP_N


def _make_reading_folder(pdf_path: Path):
    pdf_stem = pdf_path.stem
    safe_stem = "".join(c if (c.isalnum() or c in "-_") else "_" for c in pdf_stem)

    reading_dir = Path(f"reading_{safe_stem}")
    reading_dir.mkdir(parents=True, exist_ok=True)

    article_txt_path = reading_dir / f"{safe_stem}.txt"
    return reading_dir, safe_stem, article_txt_path


def _normalize_for_freq(s: str) -> str:
    return (s or "").strip().lower()


def _make_score_json(article_text: str, words: list[str]) -> dict:
    """
    ç”Ÿæˆä¸€ä¸ªâ€œå¯è§£é‡Šçš„ difficulty scoreâ€ï¼š
    - è¯é¢‘è¶Šä½è¶Šéš¾ï¼ˆlog ç¼©æ”¾ï¼‰
    - è¯è¶Šé•¿ç•¥å¾®è¶Šéš¾
    è¾“å‡ºèŒƒå›´å¤§è‡´åœ¨ [0, 1]ï¼Œä½ åç»­ä¹Ÿå¯ä»¥æ›¿æ¢æˆæ¨¡å‹ç½®ä¿¡åº¦ã€‚
    """
    toks = [_normalize_for_freq(w) for w in article_text.split()]
    freq = Counter(toks)

    # ä¸ºäº†æ›´ç¨³ï¼šå¦‚æœ split å¤ªç²—ç³™å¯¼è‡´é¢‘ç‡å…¨æ˜¯ 0ï¼Œä¹Ÿä¸ä¼šå´©
    max_f = 1
    for w in words:
        max_f = max(max_f, freq.get(_normalize_for_freq(w), 1))

    scores = {}
    for w in words:
        ww = _normalize_for_freq(w)
        f = max(1, freq.get(ww, 1))
        # rare_score: è¶Šå°‘è§è¶Šæ¥è¿‘ 1
        rare_score = 1.0 - (math.log(f + 1.0) / math.log(max_f + 1.0))
        # len_score: è¶Šé•¿è¶Šæ¥è¿‘ 1
        len_score = min(1.0, len(ww) / 12.0)
        # åˆæˆï¼ˆä½ å¯è°ƒæƒé‡ï¼‰
        s = 0.65 * rare_score + 0.35 * len_score
        scores[w] = round(float(s), 3)

    return scores


def run_ai_mode(pdf_path: Path,
                reading_dir: Path,
                safe_stem: str,
                ai_top_n: int = DEFAULT_TOP_N):
    """
    ä½¿ç”¨ AI æ¨¡å¼ï¼š
      PDF -> txt -> AI é€‰è¯è¡¨ -> words.txt + csv + json
    å…¨éƒ¨è¾“å‡ºåˆ° reading_dir ä¸‹é¢ã€‚
    """
    # 1) PDF -> TXT
    article_txt = reading_dir / f"{safe_stem}.txt"
    print(f"\n[Step 1] ä» PDF æå–æ–‡æœ¬ï¼š{pdf_path} -> {article_txt}")
    ok = extract_text_from_pdf(str(pdf_path), str(article_txt))
    if not ok:
        raise SystemExit("âŒ PDF æ–‡æœ¬æå–å¤±è´¥ï¼Œé€€å‡ºã€‚")

    text = article_txt.read_text(encoding="utf-8", errors="ignore")

    # 2) AI é€‰è¯ï¼ˆç”Ÿæˆä¸€ä¸ªç­‰ä»·äº select_XXXX.txt çš„è¯è¡¨ï¼‰
    select_path = reading_dir / f"{safe_stem}.ai.select.txt"
    print(f"\n[Step 2] ç”¨ AI ä»æ–‡ç« ç”Ÿæˆç”Ÿè¯è¡¨ï¼ˆæœ€å¤š {ai_top_n} ä¸ªè¯ï¼‰...")
    words = ai_select_words_for_article(text, top_n=ai_top_n)
    select_path.write_text("\n".join(words) + "\n", encoding="utf-8")
    print(f"  âœ“ AI é€‰è¯è¡¨å·²ä¿å­˜ï¼š{select_path} ï¼ˆå…± {len(words)} ä¸ªè¯ï¼‰")

    # 2.5) [æ–°å¢] ç”Ÿæˆ difficulty score JSONï¼ˆä¾› build_vocab_combined ä½¿ç”¨ï¼‰
    score_json_path = reading_dir / f"{safe_stem}.selected_words.json"
    scores = _make_score_json(text, words)
    score_json_path.write_text(json.dumps(scores, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"  âœ“ difficulty score JSON å·²ä¿å­˜ï¼š{score_json_path}")

    # 3) build_vocabï¼šæ–‡ç«  txt + AI è¯è¡¨ -> words.txt + csv
    out_words = reading_dir / f"{safe_stem}.ai.words.txt"
    out_csv = reading_dir / f"{safe_stem}.ai.csv"

    print(f"\n[Step 3] æ„å»ºè¯æ±‡è¡¨ CSVï¼ˆæŸ¥é‡Šä¹‰ + ä¾‹å¥ï¼‰...")
    # âœ… å…¼å®¹æ–°æ—§ build_vocab ç­¾åï¼šæ–°ç‰ˆå¸¦ score_json_pathï¼Œç¬¬äº”ä¸ªå‚æ•°
    try:
        build_vocab(str(article_txt), str(select_path), str(out_words), str(out_csv), str(score_json_path))
    except TypeError:
        build_vocab(str(article_txt), str(select_path), str(out_words), str(out_csv))

    print(f"  âœ“ æ–‡ç« ä¸­å‡ºç°çš„ç›®æ ‡è¯åˆ—è¡¨ï¼š{out_words}")
    print(f"  âœ“ è¯æ±‡ CSVï¼š{out_csv}")

    # 4) CSV -> JSON
    out_json = reading_dir / f"{safe_stem}.ai.json"
    print(f"\n[Step 4] æŠŠ CSV è½¬æˆ JSONï¼š{out_csv} -> {out_json}")
    csv_to_json(str(out_csv), str(out_json))
    print(f"  âœ“ è¯æ±‡ JSONï¼š{out_json}")

    print("\nâœ… AI æ¨¡å¼å®Œæ•´ç»“æŸï¼Œæ‰€æœ‰æ–‡ä»¶éƒ½åœ¨ï¼š", reading_dir)


def run_list_mode(pdf_path: Path,
                  reading_dir: Path,
                  safe_stem: str,
                  select_path: Path):
    """
    ä½¿ç”¨æ‰‹å·¥åˆ—è¡¨æ¨¡å¼ï¼š
      PDF -> txt
      txt + ç”¨æˆ·æä¾›çš„ select.txt -> words.txt + csv + json
    å…¨éƒ¨è¾“å‡ºåˆ° reading_dir ä¸‹é¢ã€‚
    """
    if not select_path.exists():
        raise SystemExit(f"âŒ æä¾›çš„ select è¯è¡¨ä¸å­˜åœ¨ï¼š{select_path}")

    # 1) PDF -> TXT
    article_txt = reading_dir / f"{safe_stem}.txt"
    print(f"\n[Step 1] ä» PDF æå–æ–‡æœ¬ï¼š{pdf_path} -> {article_txt}")
    ok = extract_text_from_pdf(str(pdf_path), str(article_txt))
    if not ok:
        raise SystemExit("âŒ PDF æ–‡æœ¬æå–å¤±è´¥ï¼Œé€€å‡ºã€‚")

    # 2) build_vocabï¼šæ–‡ç«  txt + ç”¨æˆ·è¯è¡¨ -> words.txt + csv
    out_words = reading_dir / f"{safe_stem}.list.words.txt"
    out_csv = reading_dir / f"{safe_stem}.list.csv"

    print(f"\n[Step 2] ä½¿ç”¨ä½ æä¾›çš„è¯è¡¨æ„å»º CSV...")
    # list æ¨¡å¼æ²¡æœ‰ score_jsonï¼Œå°±ä¼ ç©º or ä¸ä¼ ï¼ˆå…¼å®¹ï¼‰
    try:
        build_vocab(str(article_txt), str(select_path), str(out_words), str(out_csv), "")
    except TypeError:
        build_vocab(str(article_txt), str(select_path), str(out_words), str(out_csv))

    print(f"  âœ“ æ–‡ç« ä¸­å‡ºç°çš„ç›®æ ‡è¯åˆ—è¡¨ï¼š{out_words}")
    print(f"  âœ“ è¯æ±‡ CSVï¼š{out_csv}")

    # 3) CSV -> JSON
    out_json = reading_dir / f"{safe_stem}.list.json"
    print(f"\n[Step 3] æŠŠ CSV è½¬æˆ JSONï¼š{out_csv} -> {out_json}")
    csv_to_json(str(out_csv), str(out_json))
    print(f"  âœ“ è¯æ±‡ JSONï¼š{out_json}")

    print("\nâœ… æ‰‹å·¥åˆ—è¡¨æ¨¡å¼å®Œæ•´ç»“æŸï¼Œæ‰€æœ‰æ–‡ä»¶éƒ½åœ¨ï¼š", reading_dir)


def main():
    ap = argparse.ArgumentParser(
        description="ä» PDF ç”Ÿæˆé˜…è¯»ç”¨çš„ç”Ÿè¯è¡¨ & CSV & JSONï¼ˆæ”¯æŒ AI é€‰è¯ or æ‰‹å·¥ selectï¼‰"
    )
    ap.add_argument(
        "--mode",
        choices=["ai", "list"],
        required=True,
        help="ai = ä½¿ç”¨ AI è‡ªåŠ¨é€‰è¯; list = ä½¿ç”¨ä½ æä¾›çš„ select è¯è¡¨",
    )
    ap.add_argument("--pdf", required=True, help="è¾“å…¥ PDF æ–‡ä»¶è·¯å¾„")
    ap.add_argument(
        "--select",
        help="å½“ --mode list æ—¶ï¼ŒæŒ‡å®šä½ çš„è¯è¡¨ txtï¼ˆç±»ä¼¼ 20241226.txtï¼‰",
    )
    ap.add_argument(
        "--ai_top_n",
        type=float,
        default=DEFAULT_TOP_N,
        help=(
            "AI æ¨¡å¼æ—¶é€‰å¤šå°‘ä¸ªè¯ï¼š"
            ">=1 è¡¨ç¤ºå…·ä½“æ•°é‡ï¼ˆä¾‹å¦‚ 30ï¼‰ï¼Œ"
            "0~1 è¡¨ç¤ºæ¯”ä¾‹ï¼ˆä¾‹å¦‚ 0.1 = å€™é€‰è¯çš„ 10%ï¼‰"
        ),
    )

    args = ap.parse_args()

    pdf_path = Path(args.pdf)
    if not pdf_path.exists():
        raise SystemExit(f"âŒ PDF æ–‡ä»¶ä¸å­˜åœ¨ï¼š{pdf_path}")

    reading_dir, safe_stem, article_txt_path = _make_reading_folder(pdf_path)
    print(f"ğŸ“ æœ¬æ¬¡è¾“å‡ºæ–‡ä»¶å¤¹ï¼š{reading_dir}")
    print(f"ğŸ“ æ–‡ç«  txt å°†ä¿å­˜ä¸ºï¼š{article_txt_path.name}")

    if args.mode == "ai":
        run_ai_mode(pdf_path, reading_dir, safe_stem, ai_top_n=args.ai_top_n)
    else:
        if not args.select:
            raise SystemExit("âŒ --mode list éœ€è¦æä¾› --select <ä½ çš„è¯è¡¨.txt>")
        run_list_mode(pdf_path, reading_dir, safe_stem, Path(args.select))


if __name__ == "__main__":
    main()
