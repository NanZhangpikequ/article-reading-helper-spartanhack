# -*- coding: utf-8 -*-
"""
å°è£…ï¼šåŠ è½½ï¼ˆæˆ–è‡ªåŠ¨è®­ç»ƒï¼‰å…³é”®è¯æŠ½å–æ¨¡å‹ï¼Œ
å¹¶æä¾›å¯¹ä¸€æ®µæ–‡æœ¬æŠ½å–å…³é”®è¯çš„å‡½æ•°ã€‚

è¿™ä¸€ç‰ˆä¿®æ­£äº† BERT wordpiece çš„è¿˜åŸé€»è¾‘ï¼š
- ä¼šæŠŠ "chi", "##nese", "buddhist", "monk"
  è¿˜åŸæˆ "chinese buddhist monk"
- ä¸ä¼šå†å‡ºç° "chinesebuddhistmonk" è¿™ç§è¿åœ¨ä¸€èµ·çš„æ€ªè¯
"""

import os
from typing import List

import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification

from config import MODEL_DIR, MAX_SEQ_LEN, DEFAULT_TOP_N, MIN_TOKEN_LEN

_model = None
_tokenizer = None
_device = None


def _ensure_model_loaded():
    """æ‡’åŠ è½½æ¨¡å‹ï¼šç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ‰çœŸæ­£ from_pretrainedã€‚"""
    global _model, _tokenizer, _device
    if _model is not None:
        return

    if not (os.path.isdir(MODEL_DIR) and os.listdir(MODEL_DIR)):
        # å¦‚æœæ²¡æœ‰æ¨¡å‹ç›®å½•ï¼Œåˆ™è§¦å‘è®­ç»ƒï¼ˆä¸€èˆ¬åªåœ¨æœåŠ¡å™¨è·‘ä¸€æ¬¡ï¼‰
        from train_keyword_model import train
        print("âš ï¸ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ï¼Œå°†å…ˆè®­ç»ƒä¸€ä¸ªæ¨¡å‹...")
        train()

    print(f"ğŸ”Œ Loading keyword model from: {MODEL_DIR}")
    _tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
    _device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    _model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR).to(_device)
    _model.eval()


def _merge_wordpieces_to_phrase(pieces: List[str]) -> str:
    """
    æŠŠ BERT çš„ wordpieces åˆæˆå¯è¯»çš„çŸ­è¯­ï¼š
    - ["chi", "##nese"] -> "chinese"
    - ["chinese", "buddhist", "monk"] -> "chinese buddhist monk"
    """
    words: List[str] = []
    for p in pieces:
        if p.startswith("##"):
            sub = p[2:]
            if not words:
                words.append(sub)
            else:
                words[-1] = words[-1] + sub
        else:
            words.append(p)
    return " ".join(words)


def _extract_spans_from_tokens(
    tokens: List[str],
    label_ids: List[int],
    id2label: dict
) -> List[str]:
    """
    æ ¹æ® token æ ‡ç­¾åºåˆ—ï¼ˆB/I/Oï¼‰æ‹¼å‡ºå…³é”®è¯ spanã€‚
    è¿™é‡Œä¼šä¿ç•™è¯ä¹‹é—´çš„ç©ºæ ¼ï¼Œä¸ä¼šå†å‡ºç° chinesebuddhistmonk è¿™ç§æƒ…å†µã€‚
    """
    keywords: List[str] = []
    current_pieces: List[str] = []

    special_tokens = set(
        [
            getattr(_tokenizer, "cls_token", "[CLS]"),
            getattr(_tokenizer, "sep_token", "[SEP]"),
            getattr(_tokenizer, "pad_token", "[PAD]"),
        ]
    )

    for tok, lid in zip(tokens, label_ids):
        label = id2label.get(int(lid), "O")

        # è·³è¿‡ç‰¹æ®Š token
        if tok in special_tokens or tok in _tokenizer.all_special_tokens:
            if current_pieces:
                phrase = _merge_wordpieces_to_phrase(current_pieces)
                keywords.append(phrase)
                current_pieces = []
            continue

        if label == "B":
            # å¼€å¯æ–°çš„çŸ­è¯­
            if current_pieces:
                phrase = _merge_wordpieces_to_phrase(current_pieces)
                keywords.append(phrase)
            current_pieces = [tok]
        elif label == "I" and current_pieces:
            current_pieces.append(tok)
        else:
            # O æˆ–ä¸åˆç†çš„ Iï¼šç»“æŸå½“å‰ span
            if current_pieces:
                phrase = _merge_wordpieces_to_phrase(current_pieces)
                keywords.append(phrase)
                current_pieces = []

    # æ”¶å°¾
    if current_pieces:
        phrase = _merge_wordpieces_to_phrase(current_pieces)
        keywords.append(phrase)

    # ç®€å•æ¸…ç† + å»é‡ + è¿‡æ»¤å¤ªçŸ­çš„åƒåœ¾ spanï¼ˆæ¯”å¦‚ "b"ï¼‰
    cleaned: List[str] = []
    for k in keywords:
        k = k.replace("  ", " ").strip().lower()
        if not k:
            continue
        if len(k) < 2:   # ä¸¢æ‰ç‰¹åˆ«çŸ­çš„
            continue
        if k not in cleaned:
            cleaned.append(k)

    return cleaned


def extract_keywords_from_text(text: str,
                               top_n: int = DEFAULT_TOP_N) -> List[str]:
    """
    ç›´æ¥å¯¹ä¸€æ®µè‹±æ–‡æ–‡æœ¬è·‘æ¨¡å‹ï¼Œ
    è¿”å›æ¨¡å‹è®¤ä¸ºæ˜¯å…³é”®çŸ­è¯­çš„è‹¥å¹²å€™é€‰ï¼ˆçŸ­è¯­å½¢å¼ï¼Œæ¯”å¦‚ "chinese buddhist monk"ï¼‰ã€‚
    æ³¨æ„ï¼šåœ¨ AI é€‰è¯æµç¨‹é‡Œä¼šå†æŠŠçŸ­è¯­æ‹†æˆå•è¯ã€‚
    """
    _ensure_model_loaded()
    tokenizer = _tokenizer
    model = _model
    device = _device

    encoding = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=MAX_SEQ_LEN,
    )

    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

    logits = outputs.logits  # [1, seq_len, num_labels]
    pred_ids = logits.argmax(-1).squeeze(0).tolist()

    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))
    id2label = model.config.id2label

    spans = _extract_spans_from_tokens(tokens, pred_ids, id2label)

    if top_n and len(spans) > top_n:
        spans = spans[:top_n]

    return spans
